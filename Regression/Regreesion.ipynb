{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import urllib\n",
    "import scipy.optimize\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseData(fname):\n",
    "    for l in urllib.urlopen(fname):\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print \"Reading data...\"\n",
    "data = list(parseData(\"http://jmcauley.ucsd.edu/cse255/data/beer/beer_50000.json\"))\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "print data[1]['review/taste']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Use the year ('review/timeStruct'/'year') to predict the overall rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_0 =  -39.1707489355\n",
      "theta_1 =  0.0214379785639\n",
      "MSE =  0.490043819858\n"
     ]
    }
   ],
   "source": [
    "def feature(datum):\n",
    "    feat = [1]\n",
    "    feat.append(datum['review/timeStruct']['year'])\n",
    "    return feat\n",
    "\n",
    "X = [feature(d) for d in data]\n",
    "y = [d['review/overall'] for d in data]\n",
    "theta,residuals,rank,s = numpy.linalg.lstsq(X, y)\n",
    "print 'theta_0 = ',theta[0]\n",
    "print 'theta_1 = ', theta[1]\n",
    "print 'MSE = ', residuals[0]/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. A better representation of the year variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012]\n",
      "theta_ 0  =  4.05154639175\n",
      "theta_ 1  =  0.0484536082475\n",
      "theta_ 2  =  0.133317713214\n",
      "theta_ 3  =  0.0749231325737\n",
      "theta_ 4  =  -0.0731834439493\n",
      "theta_ 5  =  -0.0672219161792\n",
      "theta_ 6  =  -0.0485172256296\n",
      "theta_ 7  =  -0.0273330482842\n",
      "theta_ 8  =  -0.0305483798005\n",
      "theta_ 9  =  -0.0228298815508\n",
      "theta_ 10  =  -0.0160672056646\n",
      "theta_ 11  =  -0.0118308912142\n",
      "theta_ 12  =  -0.0128947135945\n",
      "theta_ 13  =  -0.00857119748488\n",
      "MSE =  0.489151895211\n"
     ]
    }
   ],
   "source": [
    "yearList = list()\n",
    "for i in xrange(len(data)):\n",
    "    tmpY = data[i]['review/timeStruct']['year']\n",
    "    if tmpY not in yearList:\n",
    "        yearList.append(tmpY)\n",
    "yearList = sorted(yearList)\n",
    "print yearList\n",
    "\n",
    "def feature(datum):\n",
    "    feat = [1]\n",
    "    for i in range(13):\n",
    "        if datum['review/timeStruct']['year'] == (yearList[i]):\n",
    "            feat.append(yearList[i]-1998)\n",
    "        else:\n",
    "            feat.append(0)\n",
    "    return feat\n",
    "\n",
    "X = [feature(d) for d in data]\n",
    "y = [d['review/overall'] for d in data]\n",
    "\n",
    "theta,residuals,rank,s = numpy.linalg.lstsq(X, y)\n",
    "for i in range(len(theta)):\n",
    "    print 'theta_',i,' = ',theta[i]\n",
    "print 'MSE = ', residuals[0]/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3. Train a regressor that uses the first 11 features to predict the last feature (‘quality’)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wine_data = []\n",
    "\n",
    "with open('winequality-white.csv','r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    firstline = True\n",
    "    for row in reader:\n",
    "        if firstline:\n",
    "            firstline = False\n",
    "            continue\n",
    "        wine_data.append([float(d) for d in row[0].split(';')])\n",
    "\n",
    "# random.shuffle(wine_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_0 =  256.420279081\n",
      "theta_1 =  0.135421303397\n",
      "theta_2 =  -1.72994865552\n",
      "theta_3 =  0.102651151592\n",
      "theta_4 =  0.109038568107\n",
      "theta_5 =  -0.27677514636\n",
      "theta_6 =  0.00634332168497\n",
      "theta_7 =  3.85023977417e-05\n",
      "theta_8 =  -258.652809072\n",
      "theta_9 =  1.19540565582\n",
      "theta_10 =  0.83300628521\n",
      "theta_11 =  0.097930435274\n",
      "Training MSE =  0.602307502903\n"
     ]
    }
   ],
   "source": [
    "def wine_feature(datum):\n",
    "    feat = [1]\n",
    "    feat += datum[:11]\n",
    "    return feat\n",
    "\n",
    "train_data = wine_data[:len(wine_data)/2]\n",
    "test_data = wine_data[len(wine_data)/2:]\n",
    "\n",
    "train_X = [wine_feature(d) for d in train_data]\n",
    "train_y = [d[11] for d in train_data]\n",
    "\n",
    "test_X = [wine_feature(d) for d in test_data]\n",
    "test_y = [d[11] for d in test_data]\n",
    "\n",
    "theta,residuals,rank,s = numpy.linalg.lstsq(train_X, train_y)\n",
    "for i in range(len(theta)):\n",
    "    print 'theta_'+str(i)+' = ',theta[i]\n",
    "print 'Training MSE = ', residuals[0]/len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_MSE(a,X,y):\n",
    "    error = 0.0\n",
    "    length = len(y)\n",
    "    for i in range(length):\n",
    "        error += (y[i]-a[0]-numpy.dot(a[1:],X[i][1:])) ** 2\n",
    "    return error/length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE =  0.602307502903\n",
      "Testing MSE =  0.562457130315\n"
     ]
    }
   ],
   "source": [
    "train_MSE = compute_MSE(theta,train_X,train_y)\n",
    "test_MSE = compute_MSE(theta,test_X,test_y)\n",
    "print 'Training MSE = ', train_MSE\n",
    "print 'Testing MSE = ', test_MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. An ablation experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing feature: fixed acidity\n",
      "Testing MSE =  0.559113414376\n",
      "\n",
      "removing feature: volatile acidity\n",
      "Testing MSE =  0.596384850162\n",
      "\n",
      "removing feature: citric acid\n",
      "Testing MSE =  0.562221702812\n",
      "\n",
      "removing feature: residual sugar\n",
      "Testing MSE =  0.553625063967\n",
      "\n",
      "removing feature: chlorides\n",
      "Testing MSE =  0.562629266481\n",
      "\n",
      "removing feature: free sulfur dioxide\n",
      "Testing MSE =  0.55614081793\n",
      "\n",
      "removing feature: total sulfur dioxide\n",
      "Testing MSE =  0.562429005469\n",
      "\n",
      "removing feature: density\n",
      "Testing MSE =  0.544726553466\n",
      "\n",
      "removing feature: pH\n",
      "Testing MSE =  0.559566626382\n",
      "\n",
      "removing feature: sulphates\n",
      "Testing MSE =  0.557346349988\n",
      "\n",
      "removing feature: alcohol\n",
      "Testing MSE =  0.573214743558\n",
      "\n",
      "feature with the most additional information:  volatile acidity\n",
      "feature with least additional informaiton:  total sulfur dioxide\n"
     ]
    }
   ],
   "source": [
    "def ablation_feature(datum,ex):\n",
    "    feat = [1]\n",
    "    if ex == 0:\n",
    "        feat += datum[1:11]\n",
    "    elif ex == 10:\n",
    "        feat += datum[:10]\n",
    "    else:\n",
    "        feat += datum[:ex]\n",
    "        feat += datum[ex+1:11]\n",
    "    return feat\n",
    "\n",
    "name = [\"fixed acidity\",\"volatile acidity\",\"citric acid\",\"residual sugar\",\"chlorides\",\n",
    "        \"free sulfur dioxide\",\"total sulfur dioxide\",\"density\",\"pH\",\"sulphates\",\"alcohol\"]\n",
    "\n",
    "error_set = []\n",
    "\n",
    "for ex in range(0,11):\n",
    "    \n",
    "    train_X = [ablation_feature(d,ex) for d in train_data]\n",
    "    train_y = [d[11] for d in train_data]\n",
    "\n",
    "    test_X = [ablation_feature(d,ex) for d in test_data]\n",
    "    test_y = [d[11] for d in test_data]\n",
    "\n",
    "    theta,residuals,rank,s = numpy.linalg.lstsq(train_X, train_y)\n",
    "    \n",
    "    train_MSE = compute_MSE(theta,train_X,train_y)\n",
    "    test_MSE = compute_MSE(theta,test_X,test_y)\n",
    "    error_set.append(abs(test_MSE-0.562457130315))\n",
    "    \n",
    "    print 'removing feature:', name[ex]\n",
    "    print 'Testing MSE = ', test_MSE\n",
    "    print\n",
    "    \n",
    "min_ft = numpy.argmin(error_set)\n",
    "max_ft = numpy.argmax(error_set)\n",
    "print 'feature with the most additional information: ', name[max_ft]\n",
    "print 'feature with least additional informaiton: ', name[min_ft]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "X_train = [wine_feature(d) for d in train_data]\n",
    "y_train = [d[11]>5 for d in train_data]\n",
    "\n",
    "X_test = [wine_feature(d) for d in test_data]\n",
    "y_test = [d[11]>5 for d in test_data]\n",
    "\n",
    "clf = svm.SVC(C=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "train_predictions = clf.predict(X_train)\n",
    "test_predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of train data =  1.0\n",
      "Accuracy of test data =  0.659452837893\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(predictions,reality):\n",
    "    length = len(reality)\n",
    "    correct = 0\n",
    "    for idx in range(length):\n",
    "        if predictions[idx] == reality[idx]:\n",
    "            correct += 1\n",
    "    return (correct*1.0)/(length*1.0)\n",
    "\n",
    "print 'Accuracy of train data = ', compute_accuracy(train_predictions,y_train)\n",
    "print 'Accuracy of test data = ', compute_accuracy(test_predictions,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import exp\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inner(x,y):\n",
    "    return sum([x[i]*y[i] for i in range(len(x))])\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1 + exp(-x))\n",
    "\n",
    "# NEGATIVE Log-likelihood\n",
    "def f(theta, X, y, lam):\n",
    "    loglikelihood = 0\n",
    "    for i in range(len(X)):\n",
    "        logit = inner(X[i], theta)\n",
    "        loglikelihood -= log(1 + exp(-logit))\n",
    "        if not y[i]:\n",
    "            loglikelihood -= logit\n",
    "    for k in range(len(theta)):\n",
    "        loglikelihood -= lam * theta[k]*theta[k]\n",
    "#     print \"ll =\", loglikelihood\n",
    "    return -loglikelihood\n",
    "\n",
    "# NEGATIVE Derivative of log-likelihood\n",
    "def fprime(theta, X, y, lam):\n",
    "    dl = [0.0]*len(theta)\n",
    "    for i in range(len(X)):\n",
    "    # Fill in code for the derivative\n",
    "        logit = inner(X[i], theta)\n",
    "        for k in range(len(X[i])):\n",
    "            if not y[i]:\n",
    "                dl[k] -= X[i][k]\n",
    "            dl[k] += X[i][k]*(1.0-sigmoid(logit))\n",
    "    for k in range(len(theta)):\n",
    "        dl[k] -=  2*lam*theta[k]\n",
    "    # Negate the return value since we're doing gradient *ascent*\n",
    "    return numpy.array([-x for x in dl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final log likelihood = -1388.69601353\n",
      "Accuracy =  0.76929358922\n"
     ]
    }
   ],
   "source": [
    "def wine_feature(datum):\n",
    "#     feat = [1]\n",
    "    feat = datum[:11]\n",
    "    return feat\n",
    "\n",
    "def compute_accuracy(test,label,theta):\n",
    "    count = 0\n",
    "    for i in xrange(len(test)):\n",
    "        l = 0\n",
    "        s = numpy.dot(test[i],theta)\n",
    "        if (s > 0):\n",
    "            l = 1\n",
    "        if ( l == label[i]):\n",
    "            count += 1\n",
    "    return (count * 1.0) / (len(label) * 1.0)\n",
    "\n",
    "\n",
    "n = len(wine_data)\n",
    "\n",
    "X_train = [wine_feature(d) for d in wine_data[:n/2]]# Extract features and labels from the data\n",
    "y_train = [d[11]>5 for d in wine_data[:n/2]]\n",
    "\n",
    "X_test = [wine_feature(d) for d in wine_data[n/2:]]\n",
    "y_test = [d[11]>5 for d in wine_data[n/2:]]\n",
    "\n",
    "\n",
    "# X_train = X[:len(X)/2]\n",
    "# X_test = X[len(X)/2:]\n",
    "\n",
    "# If we wanted to split with a validation set:\n",
    "#X_valid = X[len(X)/2:3*len(X)/4]\n",
    "#X_test = X[3*len(X)/4:]\n",
    "\n",
    "# Use a library function to run gradient descent (or you can implement yourself!)\n",
    "theta,l,info = scipy.optimize.fmin_l_bfgs_b(f, [0]*len(X_train[0]), fprime, args = (X_train, y_train, 1.0))\n",
    "print \"Final log likelihood =\", -l\n",
    "# predictions = [ for ]\n",
    "print \"Accuracy = \", compute_accuracy(X_test,y_test,theta)# Compute the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.45226173e-04   5.22127320e-06   7.28036504e-06   1.13443530e-04\n",
      "   8.94378700e-07   7.11479378e-04   2.82955820e-03   2.04293012e-05\n",
      "   6.65296784e-05   1.02356116e-05   2.19267542e-04]\n"
     ]
    }
   ],
   "source": [
    "print theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
